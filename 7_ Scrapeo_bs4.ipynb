{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Web Scraping /Scrapeo (Bs4)\n",
    "<p align = 'center'>\n",
    "<img src = 'https://editor.analyticsvidhya.com/uploads/75676cover.jfif'/>\n",
    "</p>\n",
    "\n",
    "Web scraping o raspado web, es una técnica utilizada mediante programas de software para extraer información de sitios web. Usualmente, estos programas simulan la navegación de un humano en la web ya sea utilizando el protocolo HTTP manualmente, o incrustando un navegador en una aplicación.\n",
    "\n",
    "El web scraping está muy relacionado con la indexación de la web, la cual indexa la información de la web utilizando un robot y es una técnica universal adoptada por la mayoría de los motores de búsqueda. Sin embargo, el web scraping se enfoca más en la transformación de datos sin estructura en la web, como el formato HTML, en datos estructurados que pueden ser almacenados y analizados en una base de datos central, en una hoja de cálculo o en alguna otra fuente de almacenamiento. Alguno de los usos del web scraping son la comparación de precios en tiendas, la monitorización de datos relacionados con el clima de cierta región, la detección de cambios en sitios webs y la integración de datos en sitios webs. \n",
    "\n",
    "En los últimos años el web scraping se ha convertido en una técnica muy utilizada dentro del sector del posicionamiento web gracias a su capacidad de generar grandes cantidades de datos para crear contenidos de calidad.\n",
    "\n",
    "Podríamos pensar que el web scraping es nuestro recurso a falta de una API o un feed RSS. A falta de una fuente de datos, siempre podemos extraer aquello que sale por pantalla."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción desde el HTML\n",
    "\n",
    "Para scrapear necesitamos saber que pinta tiene la **estructura general** que tiene un HTML.\n",
    "\n",
    "El HTML consiste en contenido `<etiquetado>`, es como si fueran cajas de contenido, organizado de manera jerárquica:\n",
    "\n",
    "```\n",
    "<html>\n",
    "    <head>\n",
    "        <title>Titulo de la pagina</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Cabecera</h1>\n",
    "        <p>Parrafo</p>\n",
    "    </body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "$$$$\n",
    "\n",
    "Las etiquetas el HTML se pueden clasificar en varios grupos, dependiendo del tipo de contenido que posea. Estos son algunos ejemplos:\n",
    "\n",
    "+ cabecera: `<h1>`, `<h2>`, `<h3>`, `<hgroup>`...\n",
    "+ texto: `<b>`, `<p>`, `<span>`...\n",
    "+ embebido: `<audio>`, `<img>`, `<video>`...\n",
    "+ tabular: `<table>`, `<tr>`, `<td>`, `<tbody>`...\n",
    "+ secciones: `<header>`, `<section>`, `<article>`...\n",
    "+ metadata: `<meta>`, `<title>`, `<script>`...\n",
    "\n",
    "$$$$\n",
    "\n",
    "\n",
    "Las etiquetas pueden tener atributos. Por ejemplo:\n",
    " \n",
    "`<div class=\"text-monospace\" id=\"name_132\", href=\"www.example.com\"> Contenido de la pagina </div>` \n",
    "\n",
    "Esta etiqueta `div` tiene los siguientes atributos:\n",
    "\n",
    "+ class: atributo con valor \"text-monospace\". La clase no es única en la página, varios elementos pueden tener la misma clase.\n",
    "+ id: atributo con valor \"name_132\". El id de una etiqueta la identifica de manera unívoca, no puede haber dos etiquetas con el mismo id.\n",
    "+ href: atributo con valor \"www.example.com\". El href suele contener el link a otra parte de la página.\n",
    "\n",
    "Siguiendo con la analogía de las cajas, si una etiqueta de HTML es una caja, los atributos serían las pegatinas pegadas en la tapa de la caja.\n",
    "\n",
    "Conociendo cual es el contenido que queremos extraer, debemos encontrar las etiquetas que nos interesan dentro de todo el HTML de la página web."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos la herramienta **[BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "\n",
    "from bs4 import BeautifulSoup as bs #este alias es standard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WIKIPEDIA\n",
    "\n",
    "**[Países europeos según esperanza de vida](https://en.wikipedia.org/wiki/List_of_European_countries_by_life_expectancy)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://en.wikipedia.org/wiki/List_of_European_countries_by_life_expectancy' #url de la pagina a scrapear\n",
    "\n",
    "html = req.get(url).content #obtenemos el contenido de la pagina\n",
    "html[:1000] #mostramos los primeros 1000 caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(html).split('/head>')[1][:1000] #mostramos los primeros 1000 caracteres de la seccion body / Esto es una gualtrapada para ver el codigo html de la pagina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "soup = bs(html, 'html.parser') #creamos el objeto soup, es un nombre standar que se la da a este objeto\n",
    "\n",
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla\n",
    "tabla = soup.find('table') #buscamos la tabla, en este caso solo hay una. Si hubiera mas, solo nos devolveria la primera. Si queremos todas, usamos find_all\n",
    "#soup.find_all('table')\n",
    "type(tabla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filas de la tabla\n",
    "filas = tabla.find_all('tr')\n",
    "filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vamos a sacar el texto de cada fila solo para los 2 primeros elementos\n",
    "for f in filas[:3]:\n",
    "    print(type(f.text),f.text)\n",
    "    \n",
    "    print('-----------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filas_nuevas = [f.text.strip().split('\\n') for f in filas]\n",
    "\n",
    "filas_nuevas[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La primera fila es la cabecera, y hay que limpiar los espacios en blanco\n",
    "\n",
    "cabecera=filas_nuevas[0]\n",
    "cabecera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a arreglarlo\n",
    "\n",
    "final=[] #aqui vamos a guardar el resultado final limpio\n",
    "\n",
    "for fila in filas_nuevas:\n",
    "    tmp=[]\n",
    "    for caracter in fila:\n",
    "        if caracter != '':\n",
    "            tmp.append(caracter)\n",
    "    final.append(tmp)\n",
    "    \n",
    "final[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Nos hacemos un dataframe\n",
    "import pandas as pd\n",
    "\n",
    "nombres_columnas = final[0] #la primera fila es la cabecera\n",
    "\n",
    "data=final[2:]\n",
    "\n",
    "df = pd.DataFrame(data, columns=nombres_columnas)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOS HEMOS QUEDADO AQUI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo geolocalización por IP\n",
    "\n",
    "https://tools.keycdn.com/geo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Dónde estoy?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://tools.keycdn.com/geo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html=req.get(url).content\n",
    "\n",
    "soup=bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('div', {'id': 'geoResult'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla=soup.find('div', {'id': 'geoResult'})#Puedo buscar por elemento, pero ademas precisando el ID\n",
    "\n",
    "tabla.find_all('dd', {'class': 'col-8 text-monospace'}) #Elemento y añado tb la clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conexion=[e.text for e in tabla.find_all('dd', {'class': 'col-8 text-monospace'})]\n",
    "\n",
    "conexion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [e.text for e in tabla.find_all('dt')]\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {k:v for k,v in zip(keys,conexion)} #me hago un diccionario con los datos de la conexion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Búsqueda según IP**\n",
    "\n",
    "https://tools.keycdn.com/geo?host=137.255.90.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://tools.keycdn.com/geo?host=137.255.90.7' #tengo parámetros en la direccion en funcion de la IP que busque...🤔\n",
    "\n",
    "html=req.get(url).content\n",
    "\n",
    "soup=bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla=soup.find('div', {'id': 'geoResult'})\n",
    "\n",
    "tabla.find_all('dd', {'class': 'col-8 text-monospace'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla.find_all('dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'{:2.2f}'.format(12.5436363636363)  # formato en strings de numeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ip=['137.255.90.7', '255.255.90.7', '177.255.21.7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo(ip):\n",
    "    \n",
    "    url=f'https://tools.keycdn.com/geo?host={ip}' #Dinamica en fucnion de la IP que quiero buscar\n",
    "    \n",
    "    html=req.get(url).content\n",
    "\n",
    "    soup=bs(html, 'html.parser')\n",
    "    \n",
    "    tabla=soup.find('div', {'id': 'geoResult'})\n",
    "    \n",
    "    conexion=[e.text for e in tabla.find_all('dd', {'class': 'col-8 text-monospace'})]\n",
    "\n",
    "    return conexion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ip in list_ip:\n",
    "    print(f'{geo(ip)}\\n') #Le meto salto de linea para que lo veamos mas claro\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo LinkedIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos intentar scrapear Linkedin\n",
    "URL='https://www.linkedin.com/jobs/search/' #Esta es la direccion de la pagina de busqueda de empleo de Linkedin general\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A esta direccion le podemos poner parametros para afinar nuestra búsqueda.\n",
    "\n",
    "URL = 'https://www.linkedin.com/jobs/search/?keywords=data&location=Espa%C3%B1a&refresh=true'\n",
    "\n",
    "`?keywords=data` nos dice que la palabra clave que queremos buscar es 'data'\n",
    "    \n",
    "`&location=Madrid` nos dice que queremos buscar en Madrid\n",
    "    \n",
    "`&refresh=true` nos dice que queremos que nos devuelva los resultados más recientes\n",
    "\n",
    "Podemos filtrar mas nuestra búsqueda en la web y veremos que van apareciendo más ***parametros*** en la URL.\n",
    "\n",
    "`&f_TPR=r120960` nos dice que empleos de la ultima semana. El parámetro va en segundos así que 60 * 60 * 24 * número de días atrás que queremos buscar\n",
    "\n",
    "`&start={i*25}` donde i sería el número de página\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a intentar scrapear esta URL\n",
    "url = 'https://www.linkedin.com/jobs/search/?keywords=data&location=Madrid&f_TPR=r1296000&F_E=1&start=50'\n",
    "\n",
    "#Estamos buscando para Madrid, termino data, ofertas publicadas en los ultimos 7 días y que nos muestre la segunda página de resultados\n",
    "\n",
    "soup=bs(req.get(url).content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intentad sacar los ingredientes de la recete de esta sopa y obtener para cada oferta de trabajo:\n",
    "\n",
    "# - Titulo\n",
    "# - Empresa\n",
    "# - Ubicacion\n",
    "# - Link de la empresa\n",
    "# - Link de la oferta\n",
    "# - Fecha de publicacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Os recomiendo ir echando una ojeada al html y la sopa poco a poco para ver como esta estructurado\n",
    "#Lo primero que tendremos que buscar será el elemento que contiene todas las ofertas....🤔\n",
    "\n",
    "#PISTA -- base-search-card__info\n",
    "#De aqui en adelante vosotros solos, tened presente que querremos guardar toda esa info en un dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suma(*args):\n",
    "    return sum(args)\n",
    "\n",
    "suma(2, 2, 3, 45, 67, 890)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REPASO FUNCIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repaso funciones\n",
    "\n",
    "def suma(*args):\n",
    "    return sum(args)\n",
    "\n",
    "suma(2, 2, 3, 45, 67, 890)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saludar(nombre, lang='es', colega=True):\n",
    "    s=''\n",
    "    \n",
    "    if colega:\n",
    "        s='colega!!!'\n",
    "        \n",
    "    if lang=='es':\n",
    "        print('Hola {} {}'.format(nombre, s))\n",
    "        \n",
    "    else:\n",
    "        print('Hello {} buddy!!!'.format(nombre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saludar_multiple(*lst, lang='es', colega=True):\n",
    "    for e in lst:\n",
    "        saludar(e, lang, colega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saludar('Pepe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saludar(['Pepe', 'en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saludar(*['Pepe', 'en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saludar_multiple('Ana', 'Pepe', 'Juan', 'Maria', lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombres=['Ana', 'Pepe', 'Juan', 'Maria']\n",
    "\n",
    "config={'lang': 'es', 'colega': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saludar_multiple(*nombres, **config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function (*args, **kwargs):\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nos atrevemos a meter nuestro código en una función que acepte parámetros para añadir a la url de base de linkedin?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metiendo todo en una funcion para que sea mas facil de usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def linkedin(num_pages, keywords, country, n_secs = 30000, exp = 1):\n",
    "    \n",
    "    URL='https://www.linkedin.com/jobs/search/'\n",
    "\n",
    "    data=[]\n",
    "\n",
    "    for i in range(num_pages):\n",
    "\n",
    "        scrape_url=''.join([\n",
    "            URL,  # url base\n",
    "            f'?keywords={keywords}',   # palabras clave de busqueda\n",
    "            f'&location={country}',   # pais lugar\n",
    "            \n",
    "            f'&f_TPR=r{n_secs}',        # segundos atras\n",
    "            f'&F_E={exp}',            # experiencia (1,2,3)\n",
    "            f'&start={i*25}'           # numero de pagina (i)\n",
    "        ])\n",
    "\n",
    "\n",
    "        html=req.get(scrape_url).content  # el html de la pagina\n",
    "\n",
    "        soup=bs(html, 'html.parser')       # la sopa parseada\n",
    "\n",
    "        for oferta in soup.find_all('div', \n",
    "                               class_ = \"base-search-card__info\"):\n",
    "                               #class_=\"base-card base-card--link base-search-card base-search-card--link job-search-card\"):\n",
    "            # bucle para las ofertas\n",
    "            titulo = oferta.find('h3', class_=\"base-search-card__title\").text.strip()   # titulo de la ofertabase\n",
    "            # titulo = oferta.find('span', class_=\"screen-reader-text\").text.strip()   # titulo de la oferta\n",
    "\n",
    "            empresa = oferta.find('h4', class_=\"base-search-card__subtitle\").text.strip()   # nombre de la compañia\n",
    "            \n",
    "            link_comp = oferta.find('a', class_=\"hidden-nested-link\").attrs['href']  # link de la compañia\n",
    "\n",
    "            lugar = oferta.find('span', class_=\"job-search-card__location\").text.strip()  # lugar\n",
    "\n",
    "            link_ofer = oferta.find('a', class_=\"hidden-nested-link\").attrs['href']   # link de la oferta\n",
    "\n",
    "            fecha=oferta.find('time').attrs['datetime']       # fecha de publicacion\n",
    "\n",
    "            data.append({\n",
    "                'title': titulo,\n",
    "                'name': empresa,\n",
    "                'link de la compañia': link_comp,\n",
    "                'location': lugar,\n",
    "                'link de la oferta': link_ofer,\n",
    "                'datetime': fecha\n",
    "            })\n",
    "    return pd.DataFrame(data)       \n",
    "    #return (pd.DataFrame(data), scrape_url)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soporte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora que tenemos la sopa parseada, vamos a buscar los elementos que nos interesan que son los que tienen la clase 'base-card base-card--link base-search-card base-search-card--link job-search-card'\n",
    "lista_ofertas = soup.find('div', class_=\"base-search-card__info\")\n",
    "lista_ofertas\n",
    "#soup.find_all('div', class_=\"base-search-card--link job-search-card\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
