{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Web Scraping /Scrapeo (Bs4)\n",
    "<p align = 'center'>\n",
    "<img src = 'https://editor.analyticsvidhya.com/uploads/75676cover.jfif'/>\n",
    "</p>\n",
    "\n",
    "Web scraping o raspado web, es una t칠cnica utilizada mediante programas de software para extraer informaci칩n de sitios web. Usualmente, estos programas simulan la navegaci칩n de un humano en la web ya sea utilizando el protocolo HTTP manualmente, o incrustando un navegador en una aplicaci칩n.\n",
    "\n",
    "El web scraping est치 muy relacionado con la indexaci칩n de la web, la cual indexa la informaci칩n de la web utilizando un robot y es una t칠cnica universal adoptada por la mayor칤a de los motores de b칰squeda. Sin embargo, el web scraping se enfoca m치s en la transformaci칩n de datos sin estructura en la web, como el formato HTML, en datos estructurados que pueden ser almacenados y analizados en una base de datos central, en una hoja de c치lculo o en alguna otra fuente de almacenamiento. Alguno de los usos del web scraping son la comparaci칩n de precios en tiendas, la monitorizaci칩n de datos relacionados con el clima de cierta regi칩n, la detecci칩n de cambios en sitios webs y la integraci칩n de datos en sitios webs. \n",
    "\n",
    "En los 칰ltimos a침os el web scraping se ha convertido en una t칠cnica muy utilizada dentro del sector del posicionamiento web gracias a su capacidad de generar grandes cantidades de datos para crear contenidos de calidad.\n",
    "\n",
    "Podr칤amos pensar que el web scraping es nuestro recurso a falta de una API o un feed RSS. A falta de una fuente de datos, siempre podemos extraer aquello que sale por pantalla."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracci칩n desde el HTML\n",
    "\n",
    "Para scrapear necesitamos saber que pinta tiene la **estructura general** que tiene un HTML.\n",
    "\n",
    "El HTML consiste en contenido `<etiquetado>`, es como si fueran cajas de contenido, organizado de manera jer치rquica:\n",
    "\n",
    "```\n",
    "<html>\n",
    "    <head>\n",
    "        <title>Titulo de la pagina</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Cabecera</h1>\n",
    "        <p>Parrafo</p>\n",
    "    </body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "$$$$\n",
    "\n",
    "Las etiquetas el HTML se pueden clasificar en varios grupos, dependiendo del tipo de contenido que posea. Estos son algunos ejemplos:\n",
    "\n",
    "+ cabecera: `<h1>`, `<h2>`, `<h3>`, `<hgroup>`...\n",
    "+ texto: `<b>`, `<p>`, `<span>`...\n",
    "+ embebido: `<audio>`, `<img>`, `<video>`...\n",
    "+ tabular: `<table>`, `<tr>`, `<td>`, `<tbody>`...\n",
    "+ secciones: `<header>`, `<section>`, `<article>`...\n",
    "+ metadata: `<meta>`, `<title>`, `<script>`...\n",
    "\n",
    "$$$$\n",
    "\n",
    "\n",
    "Las etiquetas pueden tener atributos. Por ejemplo:\n",
    " \n",
    "`<div class=\"text-monospace\" id=\"name_132\", href=\"www.example.com\"> Contenido de la pagina </div>` \n",
    "\n",
    "Esta etiqueta `div` tiene los siguientes atributos:\n",
    "\n",
    "+ class: atributo con valor \"text-monospace\". La clase no es 칰nica en la p치gina, varios elementos pueden tener la misma clase.\n",
    "+ id: atributo con valor \"name_132\". El id de una etiqueta la identifica de manera un칤voca, no puede haber dos etiquetas con el mismo id.\n",
    "+ href: atributo con valor \"www.example.com\". El href suele contener el link a otra parte de la p치gina.\n",
    "\n",
    "Siguiendo con la analog칤a de las cajas, si una etiqueta de HTML es una caja, los atributos ser칤an las pegatinas pegadas en la tapa de la caja.\n",
    "\n",
    "Conociendo cual es el contenido que queremos extraer, debemos encontrar las etiquetas que nos interesan dentro de todo el HTML de la p치gina web."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos la herramienta **[BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "\n",
    "from bs4 import BeautifulSoup as bs #este alias es standard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WIKIPEDIA\n",
    "\n",
    "**[Pa칤ses europeos seg칰n esperanza de vida](https://en.wikipedia.org/wiki/List_of_European_countries_by_life_expectancy)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://en.wikipedia.org/wiki/List_of_European_countries_by_life_expectancy' #url de la pagina a scrapear\n",
    "\n",
    "html = req.get(url).content #obtenemos el contenido de la pagina\n",
    "html[:1000] #mostramos los primeros 1000 caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(html).split('/head>')[1][:1000] #mostramos los primeros 1000 caracteres de la seccion body / Esto es una gualtrapada para ver el codigo html de la pagina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "soup = bs(html, 'html.parser') #creamos el objeto soup, es un nombre standar que se la da a este objeto\n",
    "\n",
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla\n",
    "tabla = soup.find('table') #buscamos la tabla, en este caso solo hay una. Si hubiera mas, solo nos devolveria la primera. Si queremos todas, usamos find_all\n",
    "#soup.find_all('table')\n",
    "type(tabla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filas de la tabla\n",
    "filas = tabla.find_all('tr')\n",
    "filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vamos a sacar el texto de cada fila solo para los 2 primeros elementos\n",
    "for f in filas[:3]:\n",
    "    print(type(f.text),f.text)\n",
    "    \n",
    "    print('-----------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filas_nuevas = [f.text.strip().split('\\n') for f in filas]\n",
    "\n",
    "filas_nuevas[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La primera fila es la cabecera, y hay que limpiar los espacios en blanco\n",
    "\n",
    "cabecera=filas_nuevas[0]\n",
    "cabecera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a arreglarlo\n",
    "\n",
    "final=[] #aqui vamos a guardar el resultado final limpio\n",
    "\n",
    "for fila in filas_nuevas:\n",
    "    tmp=[]\n",
    "    for caracter in fila:\n",
    "        if caracter != '':\n",
    "            tmp.append(caracter)\n",
    "    final.append(tmp)\n",
    "    \n",
    "final[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Nos hacemos un dataframe\n",
    "import pandas as pd\n",
    "\n",
    "nombres_columnas = final[0] #la primera fila es la cabecera\n",
    "\n",
    "data=final[2:]\n",
    "\n",
    "df = pd.DataFrame(data, columns=nombres_columnas)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOS HEMOS QUEDADO AQUI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo geolocalizaci칩n por IP\n",
    "\n",
    "https://tools.keycdn.com/geo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**쮻칩nde estoy?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://tools.keycdn.com/geo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html=req.get(url).content\n",
    "\n",
    "soup=bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('div', {'id': 'geoResult'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla=soup.find('div', {'id': 'geoResult'})#Puedo buscar por elemento, pero ademas precisando el ID\n",
    "\n",
    "tabla.find_all('dd', {'class': 'col-8 text-monospace'}) #Elemento y a침ado tb la clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conexion=[e.text for e in tabla.find_all('dd', {'class': 'col-8 text-monospace'})]\n",
    "\n",
    "conexion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [e.text for e in tabla.find_all('dt')]\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {k:v for k,v in zip(keys,conexion)} #me hago un diccionario con los datos de la conexion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B칰squeda seg칰n IP**\n",
    "\n",
    "https://tools.keycdn.com/geo?host=137.255.90.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://tools.keycdn.com/geo?host=137.255.90.7' #tengo par치metros en la direccion en funcion de la IP que busque...游뱂\n",
    "\n",
    "html=req.get(url).content\n",
    "\n",
    "soup=bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla=soup.find('div', {'id': 'geoResult'})\n",
    "\n",
    "tabla.find_all('dd', {'class': 'col-8 text-monospace'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla.find_all('dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'{:2.2f}'.format(12.5436363636363)  # formato en strings de numeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ip=['137.255.90.7', '255.255.90.7', '177.255.21.7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo(ip):\n",
    "    \n",
    "    url=f'https://tools.keycdn.com/geo?host={ip}' #Dinamica en fucnion de la IP que quiero buscar\n",
    "    \n",
    "    html=req.get(url).content\n",
    "\n",
    "    soup=bs(html, 'html.parser')\n",
    "    \n",
    "    tabla=soup.find('div', {'id': 'geoResult'})\n",
    "    \n",
    "    conexion=[e.text for e in tabla.find_all('dd', {'class': 'col-8 text-monospace'})]\n",
    "\n",
    "    return conexion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ip in list_ip:\n",
    "    print(f'{geo(ip)}\\n') #Le meto salto de linea para que lo veamos mas claro\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo LinkedIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos intentar scrapear Linkedin\n",
    "URL='https://www.linkedin.com/jobs/search/' #Esta es la direccion de la pagina de busqueda de empleo de Linkedin general\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A esta direccion le podemos poner parametros para afinar nuestra b칰squeda.\n",
    "\n",
    "URL = 'https://www.linkedin.com/jobs/search/?keywords=data&location=Espa%C3%B1a&refresh=true'\n",
    "\n",
    "`?keywords=data` nos dice que la palabra clave que queremos buscar es 'data'\n",
    "    \n",
    "`&location=Madrid` nos dice que queremos buscar en Madrid\n",
    "    \n",
    "`&refresh=true` nos dice que queremos que nos devuelva los resultados m치s recientes\n",
    "\n",
    "Podemos filtrar mas nuestra b칰squeda en la web y veremos que van apareciendo m치s ***parametros*** en la URL.\n",
    "\n",
    "`&f_TPR=r120960` nos dice que empleos de la ultima semana. El par치metro va en segundos as칤 que 60 * 60 * 24 * n칰mero de d칤as atr치s que queremos buscar\n",
    "\n",
    "`&start={i*25}` donde i ser칤a el n칰mero de p치gina\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a intentar scrapear esta URL\n",
    "url = 'https://www.linkedin.com/jobs/search/?keywords=data&location=Madrid&f_TPR=r1296000&F_E=1&start=50'\n",
    "\n",
    "#Estamos buscando para Madrid, termino data, ofertas publicadas en los ultimos 7 d칤as y que nos muestre la segunda p치gina de resultados\n",
    "\n",
    "soup=bs(req.get(url).content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intentad sacar los ingredientes de la recete de esta sopa y obtener para cada oferta de trabajo:\n",
    "\n",
    "# - Titulo\n",
    "# - Empresa\n",
    "# - Ubicacion\n",
    "# - Link de la empresa\n",
    "# - Link de la oferta\n",
    "# - Fecha de publicacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Os recomiendo ir echando una ojeada al html y la sopa poco a poco para ver como esta estructurado\n",
    "#Lo primero que tendremos que buscar ser치 el elemento que contiene todas las ofertas....游뱂\n",
    "\n",
    "#PISTA -- base-search-card__info\n",
    "#De aqui en adelante vosotros solos, tened presente que querremos guardar toda esa info en un dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suma(*args):\n",
    "    return sum(args)\n",
    "\n",
    "suma(2, 2, 3, 45, 67, 890)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REPASO FUNCIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repaso funciones\n",
    "\n",
    "def suma(*args):\n",
    "    return sum(args)\n",
    "\n",
    "suma(2, 2, 3, 45, 67, 890)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saludar(nombre, lang='es', colega=True):\n",
    "    s=''\n",
    "    \n",
    "    if colega:\n",
    "        s='colega!!!'\n",
    "        \n",
    "    if lang=='es':\n",
    "        print('Hola {} {}'.format(nombre, s))\n",
    "        \n",
    "    else:\n",
    "        print('Hello {} buddy!!!'.format(nombre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saludar_multiple(*lst, lang='es', colega=True):\n",
    "    for e in lst:\n",
    "        saludar(e, lang, colega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saludar('Pepe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saludar(['Pepe', 'en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saludar(*['Pepe', 'en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saludar_multiple('Ana', 'Pepe', 'Juan', 'Maria', lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombres=['Ana', 'Pepe', 'Juan', 'Maria']\n",
    "\n",
    "config={'lang': 'es', 'colega': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saludar_multiple(*nombres, **config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function (*args, **kwargs):\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nos atrevemos a meter nuestro c칩digo en una funci칩n que acepte par치metros para a침adir a la url de base de linkedin?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metiendo todo en una funcion para que sea mas facil de usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def linkedin(num_pages, keywords, country, n_secs = 30000, exp = 1):\n",
    "    \n",
    "    URL='https://www.linkedin.com/jobs/search/'\n",
    "\n",
    "    data=[]\n",
    "\n",
    "    for i in range(num_pages):\n",
    "\n",
    "        scrape_url=''.join([\n",
    "            URL,  # url base\n",
    "            f'?keywords={keywords}',   # palabras clave de busqueda\n",
    "            f'&location={country}',   # pais lugar\n",
    "            \n",
    "            f'&f_TPR=r{n_secs}',        # segundos atras\n",
    "            f'&F_E={exp}',            # experiencia (1,2,3)\n",
    "            f'&start={i*25}'           # numero de pagina (i)\n",
    "        ])\n",
    "\n",
    "\n",
    "        html=req.get(scrape_url).content  # el html de la pagina\n",
    "\n",
    "        soup=bs(html, 'html.parser')       # la sopa parseada\n",
    "\n",
    "        for oferta in soup.find_all('div', \n",
    "                               class_ = \"base-search-card__info\"):\n",
    "                               #class_=\"base-card base-card--link base-search-card base-search-card--link job-search-card\"):\n",
    "            # bucle para las ofertas\n",
    "            titulo = oferta.find('h3', class_=\"base-search-card__title\").text.strip()   # titulo de la ofertabase\n",
    "            # titulo = oferta.find('span', class_=\"screen-reader-text\").text.strip()   # titulo de la oferta\n",
    "\n",
    "            empresa = oferta.find('h4', class_=\"base-search-card__subtitle\").text.strip()   # nombre de la compa침ia\n",
    "            \n",
    "            link_comp = oferta.find('a', class_=\"hidden-nested-link\").attrs['href']  # link de la compa침ia\n",
    "\n",
    "            lugar = oferta.find('span', class_=\"job-search-card__location\").text.strip()  # lugar\n",
    "\n",
    "            link_ofer = oferta.find('a', class_=\"hidden-nested-link\").attrs['href']   # link de la oferta\n",
    "\n",
    "            fecha=oferta.find('time').attrs['datetime']       # fecha de publicacion\n",
    "\n",
    "            data.append({\n",
    "                'title': titulo,\n",
    "                'name': empresa,\n",
    "                'link de la compa침ia': link_comp,\n",
    "                'location': lugar,\n",
    "                'link de la oferta': link_ofer,\n",
    "                'datetime': fecha\n",
    "            })\n",
    "    return pd.DataFrame(data)       \n",
    "    #return (pd.DataFrame(data), scrape_url)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soporte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora que tenemos la sopa parseada, vamos a buscar los elementos que nos interesan que son los que tienen la clase 'base-card base-card--link base-search-card base-search-card--link job-search-card'\n",
    "lista_ofertas = soup.find('div', class_=\"base-search-card__info\")\n",
    "lista_ofertas\n",
    "#soup.find_all('div', class_=\"base-search-card--link job-search-card\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
